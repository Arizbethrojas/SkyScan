{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Plane Detection Model from Voxel51 Dataset\n",
    "This notebook trains a plane detection model using transfer learning. \n",
    "Depending on the label used, it can just detect a plane or it can try to detect the model of the plane.\n",
    "A pre-trained model is used as a starting point. This means that fewer example images are needed and the training process is faster.\n",
    "\n",
    "Images are exported from a Voxel51 Dataset into TensorFlow Records.The examples in the TFRecord are based on a selected Field from the Samples in the Voxel51 dataset. The V51 Sample field you choose should have 1 or more \"detections\", which are bounding boxes with a label.\n",
    "\n",
    "From: https://colab.research.google.com/drive/1sLqFKVV94wm-lglFq_0kGo2ciM0kecWD#scrollTo=wHfsJ5nWLWh9&uniqifier=1\n",
    "\n",
    "Good stuff here too: https://www.inovex.de/blog/deep-learning-mobile-tensorflow-lite/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_name=\"mobilenet_plane_detect\" # The name for the model. All of the different directories will be based on this\n",
    "label_field = \"planebox\"  # The field from the V51 Samples around which will be used for the Labels for training.\n",
    "dataset_name = \"plane-dataset\" # The name of the V51 dataset that will be used\n",
    "\n",
    "\n",
    "# Available Model Configs (You can add more from the TF2 Model Zoo)\n",
    "MODELS_CONFIG = {\n",
    "    'ssd_mobilenet_v2': {\n",
    "        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
    "        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n",
    "        'batch_size': 24\n",
    "    },\n",
    "    'ssd_mobilenet_v2_fpnlite': {\n",
    "        'model_name': 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8',\n",
    "        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz',\n",
    "        'batch_size': 18\n",
    "    },\n",
    "    'efficientdet-d0': {\n",
    "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
    "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
    "        'batch_size': 18\n",
    "    },\n",
    "    'efficientdet-d1': {\n",
    "        'model_name': 'efficientdet_d1_coco17_tpu-32',\n",
    "        'base_pipeline_file': 'ssd_efficientdet_d1_640x640_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'efficientdet_d1_coco17_tpu-32.tar.gz',\n",
    "        'batch_size': 18\n",
    "    },\n",
    "    'efficientdet-d2': {\n",
    "        'model_name': 'efficientdet_d2_coco17_tpu-32',\n",
    "        'base_pipeline_file': 'ssd_efficientdet_d2_768x768_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'efficientdet_d2_coco17_tpu-32.tar.gz',\n",
    "        'batch_size': 18\n",
    "    },\n",
    "        'efficientdet-d3': {\n",
    "        'model_name': 'efficientdet_d3_coco17_tpu-32',\n",
    "        'base_pipeline_file': 'ssd_efficientdet_d3_896x896_coco17_tpu-32.config',\n",
    "        'pretrained_checkpoint': 'efficientdet_d3_coco17_tpu-32.tar.gz',\n",
    "        'batch_size': 18\n",
    "    }\n",
    "}\n",
    "\n",
    "# change chosen model to deploy different models \n",
    "chosen_model = 'ssd_mobilenet_v2'#'efficientdet-d0'\n",
    "\n",
    "num_steps = 40000 # The more steps, the longer the training. Increase if your loss function is still decreasing and validation metrics are increasing. \n",
    "num_eval_steps = 500 # Perform evaluation after so many steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The different directories and filenames to use\n",
    "train_record_fname = \"/tf/dataset-export/\" + training_name + \"/train/tf.records\"\n",
    "val_record_fname = \"/tf/dataset-export/\" + training_name + \"/val/tf.records\"\n",
    "val_export_dir = \"/tf/dataset-export/\" + training_name + \"/val/\"\n",
    "train_export_dir = \"/tf/dataset-export/\" + training_name + \"/train/\"\n",
    "model_export_dir = \"/tf/model-export/\" + training_name +\"/\"\n",
    "\n",
    "label_map_file = \"/tf/dataset-export/\" + training_name + \"/label_map.pbtxt\"\n",
    "\n",
    "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
    "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
    "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']\n",
    "batch_size = MODELS_CONFIG[chosen_model]['batch_size'] #if you can fit a large batch in memory, it may speed up your training\n",
    "\n",
    "pipeline_fname = '/tf/models/research/deploy/' + base_pipeline_file\n",
    "fine_tune_checkpoint = '/tf/models/research/deploy/' + model_name + '/checkpoint/ckpt-0'\n",
    "pipeline_file = '/tf/models/research/deploy/pipeline_file.config'\n",
    "model_dir = '/tf/training/'+training_name+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the different packages needed\n",
    "! apt install -y protobuf-compiler libgl1-mesa-glx wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Install TF Models\n",
    "The TF Object Detection API is available here: https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models /tf/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /tf/models/research\n",
    "ls\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "cp object_detection/packages/tf2/setup.py .\n",
    "python -m pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "import tensorflow as tf\n",
    "from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem\n",
    "from google.protobuf import text_format\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Training and Val Dataset from Voxel 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import math\n",
    "dataset = fo.load_dataset(dataset_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset content\n",
    "Here are some basic stats on the Voxel51 dataset you are going to build training the model on. \n",
    "An example of the samples is also printed out. In the Sample, make sure the *label_field* you selected has some detections in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\tDataset\\n-----------------------------------\")\n",
    "view = dataset.exists(label_field).shuffle(seed=51) # You can add additional things to the query to further refine it. eg .match_tags(\"good_box\")\n",
    "print(view)\n",
    "print(\"\\n\\n\\tExample Sample\\n-----------------------------------\")\n",
    "print(view.first())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the dataset into TFRecords\n",
    "The selected dataset samples will be exported to TensorFlow Records (TFRecords). They will be split between Training and Validation. The ratio can be adjusted below. You only need to do this once to build the dataset. If you run this a second time with the same **model_name** additional samples will be appended to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dataset or DatasetView to export\n",
    "sample_len = len(view)\n",
    "val_len = math.floor(sample_len * 0.2)\n",
    "train_len = math.floor(sample_len * 0.8)\n",
    "print(\"Total: {} Val: {} Train: {}\".format(sample_len,val_len,train_len))\n",
    "val_view = view.take(val_len)\n",
    "train_view = view.skip(val_len).take(train_len)\n",
    "# Export the dataset\n",
    "val_view.export(\n",
    "    export_dir=val_export_dir,\n",
    "    dataset_type=fo.types.TFObjectDetectionDataset,#fo.types.COCODetectionDataset,#fo.types.TFObjectDetectionDataset,\n",
    "    label_field=label_field,\n",
    ")\n",
    "\n",
    "train_view.export(\n",
    "    export_dir=train_export_dir,\n",
    "    dataset_type=fo.types.TFObjectDetectionDataset,#fo.types.COCODetectionDataset,#fo.types.TFObjectDetectionDataset,\n",
    "    label_field=label_field,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a file with the Labels for the objects\n",
    "The TF2 Object Detection API looks for a map of the labels used and a corresponding Id. You can build a list of the unique classnames by itterating the dataset. You can also just hardcode it if there only a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_classes(classes, start=1):\n",
    "    msg = StringIntLabelMap()\n",
    "    for id, name in enumerate(classes, start=start):\n",
    "        msg.item.append(StringIntLabelMapItem(id=id, name=name))\n",
    "\n",
    "    text = str(text_format.MessageToBytes(msg, as_utf8=True), 'utf-8')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If labelfield is a classification\n",
    "class_names=[]\n",
    "for sample in view.select_fields(label_field):\n",
    "    if sample[label_field].label not in class_names:\n",
    "        class_names.append(sample[label_field].label)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If labelfield is detections\n",
    "class_names=[]\n",
    "for sample in view.select_fields(label_field):\n",
    "    for detection in sample[label_field].detections:\n",
    "        label = detection[\"label\"]\n",
    "        if label not in class_names:\n",
    "            class_names.append(label)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can hard wire it too\n",
    "class_names=[\"plane\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "txt = convert_classes(class_names)\n",
    "print(txt)\n",
    "with open(label_map_file, 'w') as f:\n",
    "        f.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a pretrained Model & default Config\n",
    "A list of the models can be found here: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n",
    "\n",
    "The configs are here: https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download pretrained weights\n",
    "%mkdir /tf/models/research/deploy/\n",
    "%cd /tf/models/research/deploy/\n",
    "import tarfile\n",
    "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
    "\n",
    "!wget {download_tar}\n",
    "tar = tarfile.open(pretrained_checkpoint)\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download base training configuration file\n",
    "%cd /tf/models/research/deploy\n",
    "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
    "!wget {download_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /tf/models/research/object_detection/dataset_tools/create_pet_tf_record.py \\\n",
    "    --label_map_path=\"/tf/dataset-export/pet/pet_label_map.pbtxt\" \\\n",
    "    --data_dir=\"/tf/dataset-export/pet/\" \\\n",
    "    --output_dir=\"/tf/dataset-export/pet/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Config for training\n",
    "The default config for the model being trained needs to be updated with the correct parameters and paths to the data. This just adds some standard settings, you may need to do some additional tuning if the training is not working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the total number of classes from the Label Map\n",
    "\n",
    "def get_num_classes(pbtxt_fname):\n",
    "    from object_detection.utils import label_map_util\n",
    "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
    "    categories = label_map_util.convert_label_map_to_categories(\n",
    "        label_map, max_num_classes=90, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    return len(category_index.keys())\n",
    "num_classes = get_num_classes(label_map_file)\n",
    "print(\"working with {} classes\".format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to adjust the learning rate section below. The number used here are from the EfficentDet config. I noticed that this learning rate worked well for the small bounding boxes I was using when planes were at a high altitude. You can try increasing it if the planes take up more of the image. If the initial loss rates are high (>0) that is a probably a sign that you should adjust the Learning Rate.\n",
    "\n",
    "You may also want to look at other aspects of the config file. They set the parameters for the model training and may need to be adjusted based on the Model Architecture you are using and the images you are training on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write custom configuration file by slotting our dataset, model checkpoint, and training parameters into the base pipeline file\n",
    "\n",
    "import re\n",
    "\n",
    "%cd /tf/models/research/deploy\n",
    "print('writing custom configuration file')\n",
    "\n",
    "with open(pipeline_fname) as f:\n",
    "    s = f.read()\n",
    "with open('pipeline_file.config', 'w') as f:\n",
    "    \n",
    "    # fine_tune_checkpoint\n",
    "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
    "    \n",
    "    # tfrecord files train and test.\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n",
    "\n",
    "    # label_map_path\n",
    "    s = re.sub(\n",
    "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_file), s)\n",
    "\n",
    "    # Set training batch_size.\n",
    "    s = re.sub('batch_size: [0-9]+',\n",
    "               'batch_size: {}'.format(batch_size), s)\n",
    "\n",
    "    # Set training steps, num_steps\n",
    "    s = re.sub('num_steps: [0-9]+',\n",
    "               'num_steps: {}'.format(num_steps), s)\n",
    "    \n",
    "    # Set learning_rate_base in learning_rate, sane default\n",
    "    s = re.sub('learning_rate_base: [.0-9]+',\n",
    "               'learning_rate_base: {}'.format(\"8e-2\"), s)\n",
    "    \n",
    "    # Set warmup_learning_rate in learning_rate, sane default\n",
    "    s = re.sub('warmup_learning_rate: [.0-9]+',\n",
    "               'warmup_learning_rate: {}'.format(.001), s)\n",
    "    \n",
    "    # Set warmup_steps in learning_rate, sane default\n",
    "    s = re.sub('warmup_steps: [.0-9]+',\n",
    "               'warmup_steps: {}'.format(2500), s)\n",
    "    \n",
    "    # Set total_steps in learning_rate, num_steps\n",
    "    s = re.sub('total_steps: [0-9]+',\n",
    "               'total_steps: {}'.format(num_steps), s)\n",
    "    \n",
    "    # Set number of classes num_classes.\n",
    "    s = re.sub('num_classes: [0-9]+',\n",
    "               'num_classes: {}'.format(num_classes), s)\n",
    "    s = re.sub('random_scale_crop_and_pad_to_square {\\s+output_size: 896\\s+scale_min: 0.1\\s+scale_max: 2.0\\s+}',\n",
    "               'random_crop_image {\\n\\tmin_object_covered: 1.0\\n\\tmin_aspect_ratio: 0.75\\n\\tmax_aspect_ratio: 1.5\\n\\tmin_area: 0.25\\n\\tmax_area: 0.875\\n\\toverlap_thresh: 0.5\\n\\trandom_coef: 0.125\\n}',s, flags=re.MULTILINE)\n",
    "    \n",
    "    s = re.sub('ssd_random_crop {\\s+}',\n",
    "               'random_crop_image {\\n\\tmin_object_covered: 1.0\\n\\tmin_aspect_ratio: 0.75\\n\\tmax_aspect_ratio: 1.5\\n\\tmin_area: 0.10\\n\\tmax_area: 0.75\\n\\toverlap_thresh: 0.5\\n\\trandom_coef: 0.125\\n}',s, flags=re.MULTILINE)\n",
    "    \n",
    "    \n",
    "    #fine-tune checkpoint type\n",
    "    s = re.sub(\n",
    "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
    "        \n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat /tf/models/research/deploy/pipeline_file.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom TF2 Object Detector\n",
    "\n",
    "This step will launch the TF2 Object Detection training. It can take a while to start-up. \n",
    "If you get an error about not finding the GPU, try shutting down the Jupyter kernel and restarting it.\n",
    "While it is running, it should print out the Current Loss and which Step it is on.\n",
    "\n",
    "* pipeline_file: defined above in writing custom training configuration\n",
    "* model_dir: the location tensorboard logs and saved model checkpoints will save to\n",
    "* num_train_steps: how long to train for\n",
    "* num_eval_steps: perform eval on validation set after this many steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /tf/models/research/object_detection/model_main_tf2.py \\\n",
    "    --pipeline_config_path={pipeline_file} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={num_steps} \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --num_eval_steps={num_eval_steps} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate trained model\n",
    "After the model has finished training, try running it against some data to see if it atleast works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io, os, glob\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "  \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "  Puts image into numpy array to feed into tensorflow graph.\n",
    "  Note that by convention we put it into a numpy array with shape\n",
    "  (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "  Args:\n",
    "    path: the file path to the image\n",
    "\n",
    "  Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3)\n",
    "  \"\"\"\n",
    "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "  image = Image.open(BytesIO(img_data))\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls {model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model from a training checkpoint\n",
    "Select a checkpoint index from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generally you want to put the last ckpt index from training in here\n",
    "checkpoint_index=41\n",
    "\n",
    "# recover our saved model\n",
    "pipeline_config = pipeline_file\n",
    "\n",
    "checkpoint = model_dir + \"ckpt-\" + str(checkpoint_index)\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(checkpoint)).expect_partial()\n",
    "\n",
    "\n",
    "def get_model_detection_function(model):\n",
    "  \"\"\"Get a tf.function for detection.\"\"\"\n",
    "\n",
    "  @tf.function\n",
    "  def detect_fn(image):\n",
    "    \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "    image, shapes = model.preprocess(image)\n",
    "    prediction_dict = model.predict(image, shapes)\n",
    "    detections = model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
    "\n",
    "  return detect_fn\n",
    "\n",
    "detect_fn = get_model_detection_function(detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels for inference decoding\n",
    "label_map_path = configs['eval_input_config'].label_map_path\n",
    "label_map = label_map_util.load_labelmap(label_map_path)\n",
    "categories = label_map_util.convert_label_map_to_categories(\n",
    "    label_map,\n",
    "    max_num_classes=label_map_util.get_max_label_map_index(label_map),\n",
    "    use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run detector on test image\n",
    "#it takes a little longer on the first run and then runs at normal speed. \n",
    "import random\n",
    "\n",
    "TEST_IMAGE_PATHS = glob.glob('/tf/testing/Airbus A321-271NX/*.jpg') #/tf/dataset-export/pet/images/keeshond_171.jpg') #'/tf/testing/Dassault Aviation FALCON 2000/*.jpg')\n",
    "image_path = random.choice(TEST_IMAGE_PATHS)\n",
    "image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "\n",
    "\n",
    "print(detections['detection_scores'])\n",
    "label_id_offset = 1 # Depending on whether your LabelMap starts at 0 or 1\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np_with_detections,\n",
    "      detections['detection_boxes'][0].numpy(),\n",
    "      (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "      detections['detection_scores'][0].numpy(),\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=200,\n",
    "      min_score_thresh=.2,\n",
    "      agnostic_mode=False,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20,25))\n",
    "plt.imshow(image_np_with_detections)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the model\n",
    "When you have a working model, use the TF2 Object Detection API to export it to a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /tf/models/research/object_detection/exporter_main_v2.py \\\n",
    "    --input_type image_tensor \\\n",
    "    --trained_checkpoint_dir={model_dir} \\\n",
    "    --pipeline_config_path={pipeline_file} \\\n",
    "    --output_directory {model_export_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export a TFLite compatible model\n",
    "Remeber that only Detection models that use SSDs are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /tf/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
    "  --pipeline_config_path={pipeline_file} \\\n",
    "  --trained_checkpoint_dir={model_dir} \\\n",
    "  --output_directory={model_export_dir}tflite-compatible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tflite_convert \\\n",
    "    --saved_model_dir=\"{model_export_dir}tflite/saved_model\" \\\n",
    "    --output_file=\"{model_export_dir}output.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tensorflow/models/issues/9033#issuecomment-706573546\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "train_images = []\n",
    "\n",
    "def representative_data_gen():\n",
    "    path = '/tf/testing/Airbus A319-115'\n",
    "\n",
    "    dataset_list = tf.data.Dataset.list_files(path + '/*.jpg')\n",
    "    for i in range(100):\n",
    "        image = next(iter(dataset_list))\n",
    "        image = tf.io.read_file(image)\n",
    "        image = tf.io.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [300, 300])\n",
    "        image = tf.cast(image / 255., tf.float32)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        yield [image]\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_export_dir+\"tflite-compatible/saved_model\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n",
    "                                       tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "#converter.inference_input_type = tf.int8\n",
    "#converter.inference_output_type = tf.int8\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "#converter.inference_input_type = tf.uint8\n",
    "#converter.inference_output_type = tf.uint8\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(model_export_dir+'model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "!apt-get update\n",
    "!apt-get -y install edgetpu-compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!edgetpu_compiler -s {model_export_dir}model.tflite -o {model_export_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export a TensorJS compatible model\n",
    "From: https://www.tensorflow.org/js/tutorials/conversion/import_saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorflowjs_converter \\\n",
    "    --input_format=tf_saved_model \\\n",
    "    {model_export_dir}saved_model \\\n",
    "    {model_export_dir}web_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir /tf/models/research/deploy/ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model --all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}