{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning any CNN image classifier into an object detector with Keras, TensorFlow, and OpenCV\n",
    "From: https://www.pyimagesearch.com/2020/06/22/turning-any-cnn-image-classifier-into-an-object-detector-with-keras-tensorflow-and-opencv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install imutils opencv-contrib-python\n",
    "! apt install -y libgl1-mesa-glx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "import imutils\n",
    "def sliding_window(image, step, ws):\n",
    "\t# slide a window across the image\n",
    "\tfor y in range(0, image.shape[0] - ws[1], step):\n",
    "\t\tfor x in range(0, image.shape[1] - ws[0], step):\n",
    "\t\t\t# yield the current window\n",
    "\t\t\tyield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "\t# yield the original image\n",
    "\tyield image\n",
    "\t# keep looping over the image pyramid\n",
    "\twhile True:\n",
    "\t\t# compute the dimensions of the next image in the pyramid\n",
    "\t\tw = int(image.shape[1] / scale)\n",
    "\t\timage = imutils.resize(image, width=w)\n",
    "\t\t# if the resized image does not meet the supplied minimum\n",
    "\t\t# size, then stop constructing the pyramid\n",
    "\t\tif image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "\t\t\tbreak\n",
    "\t\t# yield the next image in the pyramid\n",
    "\t\tyield image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# image folder\n",
    "folder_path = '/tf/testing'\n",
    "# path to model\n",
    "model_path = '/tf/dataset/plane-detector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our network weights from disk\n",
    "print(\"[INFO] loading network...\")\n",
    "# load the trained model\n",
    "model = load_model(model_path)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables used for the object detection procedure\n",
    "\n",
    "WIDTH = 600\n",
    "PYR_SCALE = 1.5\n",
    "WIN_STEP = 10\n",
    "ROI_SIZE = (20, 20) #(200,150)\n",
    "INPUT_SIZE = (299, 299)\n",
    "MIN_CONF = 1\n",
    "image=\"./sample.jpg\"\n",
    "VISUALIZE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input image from disk, resize it such that it has the\n",
    "# has the supplied width, and then grab its dimensions\n",
    "orig = cv2.imread(image)\n",
    "orig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "#cv2.imshow(\"ROI\", orig)\n",
    "orig = imutils.resize(orig, width=WIDTH)\n",
    "(H, W) = orig.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the image pyramid\n",
    "pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "# initialize two lists, one to hold the ROIs generated from the image\n",
    "# pyramid and sliding window, and another list used to store the\n",
    "# (x, y)-coordinates of where the ROI was in the original image\n",
    "rois = []\n",
    "locs = []\n",
    "# time how long it takes to loop over the image pyramid layers and\n",
    "# sliding window locations\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the image pyramid\n",
    "for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "    scale = W / float(image.shape[1])\n",
    "    # for each layer of the image pyramid, loop over the sliding\n",
    "    # window locations\n",
    "    for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "        # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "        # *original* image dimensions\n",
    "        x = int(x * scale)\n",
    "        y = int(y * scale)\n",
    "        w = int(ROI_SIZE[0] * scale)\n",
    "        h = int(ROI_SIZE[1] * scale)\n",
    "        # take the ROI and preprocess it so we can later classify\n",
    "        # the region using Keras/TensorFlow\n",
    "        roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "        roi = img_to_array(roi)\n",
    "        #roi = preprocess_input(roi)\n",
    "        # update our list of ROIs and associated coordinates\n",
    "        rois.append(roi)\n",
    "        locs.append((x, y, x + w, y + h))\n",
    "        # check to see if we are visualizing each of the sliding\n",
    "        # windows in the image pyramid\n",
    "        if VISUALIZE:\n",
    "            # clone the original image and then draw a bounding box\n",
    "            # surrounding the current region\n",
    "            clone = orig.copy()\n",
    "            cv2.rectangle(clone, (x, y), (x + w, y + h),\n",
    "                (0, 255, 0), 2)\n",
    "            # show the visualization and current ROI\n",
    "            plt.imshow(clone)\n",
    "            plt.title(\"Visualization\")\n",
    "            plt.show()\n",
    "            plt.imshow(roiOrig)\n",
    "            plt.title(\"roiOrig\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show how long it took to loop over the image pyramid layers and\n",
    "# sliding window locations\n",
    "end = time.time()\n",
    "print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "\tend - start))\n",
    "# convert the ROIs to a NumPy array\n",
    "#rois = (rois[...,::-1].astype(np.float32)) / 255.0\n",
    "rois = np.array(rois, dtype=\"float32\")\n",
    "# classify each of the proposal ROIs using ResNet and then show how\n",
    "# long the classifications took\n",
    "print(\"[INFO] classifying ROIs...\")\n",
    "start = time.time()\n",
    "preds = model.predict(rois)\n",
    "end = time.time()\n",
    "print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "\tend - start))\n",
    "# decode the predictions and initialize a dictionary which maps class\n",
    "# labels (keys) to any ROIs associated with that label (values)\n",
    "#preds = imagenet_utils.decode_predictions(preds, top=1)\n",
    "#print(rois.shape)\n",
    "#print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(preds)\n",
    "# loop over the predictions\n",
    "for (i, p) in enumerate(preds):\n",
    "    # grab the prediction information for the current ROI\n",
    "    #(imagenetID, label, prob) = p[0]\n",
    "    prob = p[0]\n",
    "    # filter out weak detections by ensuring the predicted probability\n",
    "    # is greater than the minimum probability\n",
    "    if prob >= MIN_CONF:\n",
    "        # grab the bounding box associated with the prediction and\n",
    "        # convert the coordinates\n",
    "        box = locs[i]\n",
    "        # grab the list of predictions for the label and add the\n",
    "        # bounding box and probability to the list\n",
    "        #L = labels.get(label, [])\n",
    "        #L = \"plane\" #labels.get(label, [])\n",
    "        L=[]\n",
    "        L.append((box, prob))\n",
    "        labels[\"plane\"] = L\n",
    "print(L)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the labels for each of detected objects in the image\n",
    "for label in labels.keys():\n",
    "    # clone the original image so that we can draw on it\n",
    "    print(\"[INFO] showing results for '{}'\".format(label))\n",
    "    print(labels[label])\n",
    "    clone = orig.copy()\n",
    "    # loop over all bounding boxes for the current label\n",
    "    for (box, prob) in labels[label]:\n",
    "        # draw the bounding box on the image\n",
    "        (startX, startY, endX, endY) = box\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "    # show the results *before* applying non-maxima suppression, then\n",
    "    # clone the image again so we can display the results *after*\n",
    "    # applying non-maxima suppression\n",
    "    plt.imshow(clone)\n",
    "    plt.title('Before')\n",
    "    plt.show()\n",
    "    clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "    # show the output after apply non-maxima suppression\n",
    "    plt.imshow(clone)\n",
    "    plt.title('After')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Search approach for suggesting ROI\n",
    "From: https://www.pyimagesearch.com/2020/06/29/opencv-selective-search-for-object-detection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST=True\n",
    "FILE_PATH=\"./sample.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input image\n",
    "image = cv2.imread(FILE_PATH)\n",
    "# initialize OpenCV's selective search implementation and set the\n",
    "# input image\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "ss.setBaseImage(image)\n",
    "# check to see if we are using the *fast* but *less accurate* version\n",
    "# of selective search\n",
    "if FAST:\n",
    "    print(\"[INFO] using *fast* selective search\")\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "# otherwise we are using the *slower* but *more accurate* version\n",
    "else:\n",
    "    ss.switchToSelectiveSearchQuality()\n",
    "    print(\"[INFO] using *quality* selective search\")\n",
    "# run selective search on the input image\n",
    "start = time.time()\n",
    "rects = ss.process()\n",
    "end = time.time()\n",
    "# show how along selective search took to run along with the total\n",
    "# number of returned region proposals\n",
    "print(\"[INFO] selective search took {:.4f} seconds\".format(end - start))\n",
    "print(\"[INFO] {} total region proposals\".format(len(rects)))\n",
    "plt.imshow(image)\n",
    "plt.title('image')\n",
    "plt.show()\n",
    "\n",
    "# loop over the region proposals in chunks (so we can better\n",
    "# visualize them)\n",
    "for i in range(0, len(rects), 100):\n",
    "    # clone the original image so we can draw on it\n",
    "    output = image.copy()\n",
    "    # loop over the current subset of region proposals\n",
    "    for (x, y, w, h) in rects[i:i + 100]:\n",
    "        # draw the region proposal bounding box on the image\n",
    "        color = [random.randint(0, 255) for j in range(0, 3)]\n",
    "        cv2.rectangle(output, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "plt.imshow(output)\n",
    "plt.title('Output')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding ROIs to a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "import numpy as np\n",
    "# image folder\n",
    "folder_path = '/tf/testing'\n",
    "# path to model\n",
    "model_path = '/tf/dataset/plane-detector'\n",
    "INPUT_SIZE = (299, 299)\n",
    "MIN_CONF = 1\n",
    "FAST = True\n",
    "FAST=True\n",
    "FILE_PATH=\"./sample.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our network weights from disk\n",
    "print(\"[INFO] loading network...\")\n",
    "# load the trained model\n",
    "model = load_model(model_path)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search(image, fast):\n",
    "\t# initialize OpenCV's selective search implementation and set the\n",
    "\t# input image\n",
    "\tss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\tss.setBaseImage(image)\n",
    "\t# check to see if we are using the *fast* but *less accurate* version\n",
    "\t# of selective search\n",
    "\tif fast:\n",
    "\t\tss.switchToSelectiveSearchFast()\n",
    "\t# otherwise we are using the *slower* but *more accurate* version\n",
    "\telse:\n",
    "\t\tss.switchToSelectiveSearchQuality()\n",
    "\t# run selective search on the input image\n",
    "\trects = ss.process()\n",
    "\t# return the region proposal bounding boxes\n",
    "\treturn rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input image\n",
    "image = cv2.imread(FILE_PATH)\n",
    "(H, W) = image.shape[:2]\n",
    "rects = selective_search(image, FAST)\n",
    "print(\"[INFO] {} regions found by selective search\".format(len(rects)))\n",
    "# initialize the list of region proposals that we'll be classifying\n",
    "# along with their associated bounding boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the region proposals in chunks (so we can better\n",
    "# visualize them)\n",
    "for i in range(0, len(rects), 100):\n",
    "    # clone the original image so we can draw on it\n",
    "    output = image.copy()\n",
    "    # loop over the current subset of region proposals\n",
    "    for (x, y, w, h) in rects[i:i + 100]:\n",
    "        # draw the region proposal bounding box on the image\n",
    "        color = [random.randint(0, 255) for j in range(0, 3)]\n",
    "        cv2.rectangle(output, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "plt.imshow(output)\n",
    "plt.title('Output')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals = []\n",
    "boxes = []\n",
    "# loop over the region proposal bounding box coordinates generated by\n",
    "# running selective search\n",
    "for (x, y, w, h) in rects:\n",
    "    # if the width or height of the region is less than 10% of the\n",
    "    # image width or height, ignore it (i.e., filter out small\n",
    "    # objects that are likely false-positives)\n",
    "    if w / float(W) > 0.2 or h / float(H) > 0.2:\n",
    "        continue\n",
    "    if w / float(W) < 0.05 or h / float(H) < 0.05:\n",
    "        continue\n",
    "    # extract the region from the input image, convert it from BGR to\n",
    "    # RGB channel ordering, and then resize it to 224x224 (the input\n",
    "    # dimensions required by our pre-trained CNN)\n",
    "    \n",
    "\n",
    "    roi = image[y:y + h, x:x + w]\n",
    "    \n",
    "    # Crop, but add a border\n",
    "    old_size = roi.shape[:2]\n",
    "    ratio = float(299)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    roi = cv2.resize(roi, (new_size[1], new_size[0]))\n",
    "\n",
    "    delta_w = 398 - new_size[1] # this gets us back to the original image ratio size from taining\n",
    "    delta_h = 299 - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "    color = [0, 0, 0]\n",
    "    roi = cv2.copyMakeBorder(roi, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "\n",
    "    roi = cv2.resize(roi, (299, 299))\n",
    "\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "    roi = img_to_array(roi)\n",
    "    #roi = preprocess_input(roi)\n",
    "    # update our proposals and bounding boxes lists\n",
    "    proposals.append(roi)\n",
    "    boxes.append((x, y, w, h))\n",
    "\n",
    "print(\"[INFO] {} boxes to invesitgate\".format(len(boxes)))\n",
    "print(\"[INFO] {} proposals to predict\".format(len(boxes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the proposals list into NumPy array and show its dimensions\n",
    "proposals = np.array(proposals, dtype=\"float32\")\n",
    "print(\"[INFO] proposal shape: {}\".format(proposals.shape))\n",
    "# classify each of the proposal ROIs using ResNet and then decode the\n",
    "# predictions\n",
    "print(\"[INFO] classifying proposals...\")\n",
    "preds = model.predict(proposals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(preds)\n",
    "# loop over the predictions\n",
    "print(\"[INFO] {} predictions\".format(len(preds)))\n",
    "labels = {\"plane\":[]}\n",
    "for (i, p) in enumerate(preds):\n",
    "    # grab the prediction information for the current ROI\n",
    "    #(imagenetID, label, prob) = p[0]\n",
    "    prob = p[0]\n",
    "    # filter out weak detections by ensuring the predicted probability\n",
    "    # is greater than the minimum probability\n",
    "    if prob >= MIN_CONF:\n",
    "        # grab the bounding box associated with the prediction and\n",
    "        # convert the coordinates\n",
    "        (x, y, w, h) = boxes[i]\n",
    "        box = (x, y, x + w, y + h)\n",
    "        #print(box)\n",
    "        # grab the list of predictions for the label and add the\n",
    "        # bounding box + probability to the list\n",
    "        #L = labels.get(\"plane\",[])\n",
    "        #L.append((box, prob))\n",
    "        labels[\"plane\"].append((box, prob))\n",
    "        \n",
    "\n",
    "print(\"[INFO] {} predicted values\".format(len(labels[\"plane\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the labels for each of detected objects in the image\n",
    "for label in labels.keys():\n",
    "    # clone the original image so that we can draw on it\n",
    "    print(\"[INFO] showing results for '{}'\".format(label))\n",
    "    clone = image.copy()\n",
    "    # loop over all bounding boxes for the current label\n",
    "    for (box, prob) in labels[label]:\n",
    "        # draw the bounding box on the image\n",
    "        if prob >5:\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "    # show the results *before* applying non-maxima suppression, then\n",
    "    # clone the image again so we can display the results *after*\n",
    "    # applying non-maxima suppression\n",
    "    plt.imshow(clone)\n",
    "    plt.title('Before')\n",
    "    plt.show()\n",
    "    clone = image.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    ind = np.argmax(proba)\n",
    "    maxBox=boxes[ind]\n",
    "\n",
    "    #print(boxes)\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "    # show the output after apply non-maxima suppression\n",
    "    plt.imshow(clone)\n",
    "    plt.title('Non Max Suppression')\n",
    "    plt.show()\n",
    "    clone = image.copy()\n",
    "    (startX, startY, endX, endY) = maxBox\n",
    "    # draw the bounding box and label on the image\n",
    "    cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "    y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "    cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "    # show the output after apply non-maxima suppression\n",
    "    plt.imshow(clone)\n",
    "    plt.title('Max Confidence')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}